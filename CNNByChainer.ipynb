{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kokubun/.pyenv/versions/3.6.2/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/kokubun/.pyenv/versions/3.6.2/envs/tensorflow/lib/python3.6/site-packages/chainer/_environment_check.py:38: UserWarning: Accelerate has been detected as a NumPy backend library.\n",
      "vecLib, which is a part of Accelerate, is known not to work correctly with Chainer.\n",
      "We recommend using other BLAS libraries such as OpenBLAS.\n",
      "For details of the issue, please see\n",
      "https://docs.chainer.org/en/stable/tips.html#mnist-example-does-not-converge-in-cpu-mode-on-mac-os-x.\n",
      "\n",
      "Also note that Chainer does not officially support Mac OS X.\n",
      "Please use it at your own risk.\n",
      "\n",
      "  ''')  # NOQA\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import chainer\n",
    "import chutil\n",
    "\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import Chain\n",
    "import chainer.optimizers as optimizers\n",
    "\n",
    "from chainer.datasets.mnist import get_mnist\n",
    "from chainer import optimizers, training\n",
    "from chainer.training import extensions\n",
    "\n",
    "# データセットがダウンロード済みでなければ、ダウンロードも行う\n",
    "train, test = get_mnist(withlabel=True, ndim=1)\n",
    "train, validation = chainer.datasets.split_dataset_random(train, 50000, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConvNet(Chain):\n",
    "    def __init__(self):\n",
    "        super(MyConvNet, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # 畳み込み層の定義\n",
    "            # in_channels:Noneを指定しても動的にメモリ確保するので問題なく動作する\n",
    "            # out_channels:出力する配列のチャンネル数\n",
    "            # ksize:フィルタのサイズ（平行移動するフィルターの長さを指定）\n",
    "            # stride:入力データに対してstride分フィルターを適用していくパラメータを指定\n",
    "            # pad:イメージは画像データの周りにpadのサイズ分だけ空白を用意してそこに対してもフィルターを適用するようなイメージ\n",
    "            # dilate:今回の実装では設定していないが、飛び飛びにフィルターを適用するパラメータ\n",
    "            self.conv1 = L.Convolution2D(\n",
    "                in_channels=None, out_channels=32, ksize=3, stride=1, pad=1)\n",
    "            # 畳み込み層の定義２層目\n",
    "            self.conv2 = L.Convolution2D(\n",
    "                in_channels=None, out_channels=64, ksize=3, stride=1, pad=1)\n",
    "            # 畳み込み層の定義３層目\n",
    "            self.conv3 = L.Convolution2D(\n",
    "                in_channels=None, out_channels=128, ksize=3, stride=1, pad=1)\n",
    "            self.conv4 = L.Convolution2D(\n",
    "                in_channels=None, out_channels=128, ksize=3, stride=1, pad=1)\n",
    "            self.fc5 = L.Linear(None, 1000)\n",
    "            self.fc6 = L.Linear(None, 10)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.sigmoid(self.conv1(x.reshape((-1, 1, 28, 28))))\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        h = F.sigmoid(self.conv2(h))\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        h = F.sigmoid(self.conv3(h))\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        h = F.sigmoid(self.conv4(h))\n",
    "        h = F.sigmoid(self.fc5(h))\n",
    "        return self.fc6(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  train_and_validate(\n",
    "        model, optimizer, train, validation, n_epoch, batchsize, device=0):\n",
    "    \n",
    "    # 1. deviceがgpuであれば、gpuにモデルのデータを転送する\n",
    "    if device >= 0:\n",
    "        model.to_cpu()\n",
    "        \n",
    "    # 2. Optimizerを設定する\n",
    "    optimizer.setup(model)\n",
    "    \n",
    "    # 3. DatasetからIteratorを作成する\n",
    "    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "    validation_iter = chainer.iterators.SerialIterator(\n",
    "        validation, batchsize, repeat=False, shuffle=False)\n",
    "    \n",
    "    # 4. Updater・Trainerを作成する\n",
    "    updater = training.StandardUpdater(train_iter, optimizer)\n",
    "    trainer = chainer.training.Trainer(updater, (n_epoch, 'epoch'), out='out')\n",
    "    \n",
    "    # 5. Trainerの機能を拡張する\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.Evaluator(validation_iter, model), name='val')\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'main/loss', 'main/accuracy', 'val/main/loss', 'val/main/accuracy', 'elapsed_time']))\n",
    "    trainer.extend(extensions.PlotReport(\n",
    "        ['main/loss', 'val/main/loss'],x_key='epoch', file_name='loss.png'))\n",
    "    trainer.extend(extensions.PlotReport(\n",
    "        ['main/accuracy', 'val/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "    trainer.extend(extensions.dump_graph('main/loss'))\n",
    "    \n",
    "    # 6. 訓練を開始する\n",
    "    trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   main/accuracy  val/main/loss  val/main/accuracy  elapsed_time\n",
      "\u001b[J1           2.3369      0.0996843      2.35329        0.112144           215.874       \n",
      "\u001b[J2           2.32517     0.100444       2.34466        0.0958267          418.639       \n",
      "\u001b[J3           1.38403     0.484175       0.234812       0.922073           619.559       \n",
      "\u001b[J4           0.145047    0.954843       0.0998211      0.969047           820.597       \n",
      "\u001b[J5           0.0903832   0.971867       0.0714812      0.978343           1023.91       \n",
      "\u001b[J6           0.0705749   0.978105       0.0686465      0.977749           1225.83       \n",
      "\u001b[J7           0.0568313   0.982017       0.0655991      0.978936           1429.8        \n",
      "\u001b[J8           0.0497249   0.984315       0.0625667      0.982002           1632.55       \n",
      "\u001b[J9           0.0416389   0.986873       0.0454976      0.985562           1835.52       \n",
      "\u001b[J10          0.0384065   0.987472       0.0460382      0.986254           2038.71       \n",
      "\u001b[J11          0.0334104   0.989403       0.0424847      0.986946           2238.91       \n",
      "\u001b[J12          0.0287021   0.990729       0.0406737      0.987836           2440.45       \n",
      "\u001b[J13          0.0236717   0.992367       0.0531063      0.984276           2802.84       \n"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "batchsize = 128\n",
    "\n",
    "model = MyConvNet()\n",
    "classifier_model = L.Classifier(model)\n",
    "optimizer = optimizers.Adam()\n",
    "train_and_validate(\n",
    "    classifier_model, optimizer, train, validation, n_epoch, batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
